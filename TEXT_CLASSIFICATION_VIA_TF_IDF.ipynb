{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a07ce6",
   "metadata": {},
   "source": [
    "# Spam Detection Project\n",
    "\n",
    "This notebook demonstrates a step-by-step approach to implementing text classification for spam detection, leveraging Term Frequency-Inverse Document Frequency (TF-IDF) and classical machine learning models. The project aligns with the requirements of our university course, emphasizing:\n",
    "- Using classical frameworks like scikit-learn for basic spam detection.\n",
    "- Exploring datasets such as the SpamAssassin public corpus.\n",
    "- Comparing classifiers like Decision Trees, Naive Bayes, and Neural Networks.\n",
    "\n",
    "This enhanced version includes additional features such as:\n",
    "- Comprehensive data exploration and preprocessing.\n",
    "- Robust evaluation with metrics like precision, recall, F1-score, and accuracy.\n",
    "- Insights into modern approaches for text classification using LLMs for context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c576fa8",
   "metadata": {},
   "source": [
    "# Text Classification using Decision Trees in Python\n",
    "\n",
    "Text classification is the process of classifying the text documents into predefined categories. In this article, we are going to explore how we can leverage decision trees to classify the textual data.\n",
    "\n",
    "Text Classification and Decision Trees\n",
    "Text classification involves assigning predefined categories or labels to text documents based on their content. Decision trees are hierarchical tree structures that recursively partition the feature space based on the values of input features. They are particularly well-suited for classification tasks due to their simplicity, interpretability, and ability to handle non-linear relationships.\n",
    "\n",
    "Decision Trees provide a clear and understandable model for text classification, making them an excellent choice for tasks where interpretability is as important as predictive power. Their inherent simplicity, however, might lead to challenges when dealing with very complex or nuanced text data, leading practitioners to explore more sophisticated or ensemble methods for improvement.\n",
    "\n",
    "Implementation: Text Classification using Decision Trees\n",
    "For text classification using Decision Trees in Python, we'll use the popular 20 Newsgroups dataset. This dataset comprises around 20,000 newsgroup documents, partitioned across 20 different newsgroups. We'll use scikit-learn to fetch the dataset, preprocess the text, convert it into a feature vector using TF-IDF vectorization, and then apply a Decision Tree classifier for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c866a",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "We will work with the SpamAssassin public corpus, which contains a collection of labeled emails for spam classification. The dataset is well-suited for demonstrating text classification tasks. The focus will be on preprocessing the dataset to extract meaningful features for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a03e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpamAssassin dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths to spam and ham emails (adjust to your dataset directory)\n",
    "spam_path = 'path_to_spam_folder'\n",
    "ham_path = 'path_to_ham_folder'\n",
    "\n",
    "# Helper function to load emails\n",
    "def load_emails(path, label):\n",
    "    emails = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r', encoding='latin-1') as file:\n",
    "            emails.append({'text': file.read(), 'label': label})\n",
    "    return emails\n",
    "\n",
    "# Load spam and ham emails\n",
    "spam_emails = load_emails(spam_path, 'spam')\n",
    "ham_emails = load_emails(ham_path, 'ham')\n",
    "\n",
    "# Create a DataFrame\n",
    "emails_df = pd.DataFrame(spam_emails + ham_emails)\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e4fa6",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Let's explore the dataset to understand the distribution of classes and the content of emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b578755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "emails_df['label'].value_counts().plot(kind='bar', title='Class Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd64b0",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "We will preprocess the text data by:\n",
    "- Removing stop words.\n",
    "- Lowercasing text.\n",
    "- Applying TF-IDF vectorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741d0bb",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "The 20 Newsgroups dataset is loaded with specific categories for simplification. Headers, footers, and quotes are removed to focus on the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4fb1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file ipython-input-1-c6b25e0bfe35\n",
    "# Import the necessary function from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60782209",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis\n",
    "\n",
    "This code snippet provides basic exploratory data analysis by visualizing the distribution of classes in the training and test sets and displaying sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file ipython-input-4-4e958edbf22e\n",
    "# Import necessary library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display distribution of classes in the training set\n",
    "# Assign the target labels from newsgroups_train to y_train\n",
    "y_train = newsgroups_train.target\n",
    "class_distribution = np.bincount(y_train)\n",
    "plt.bar(range(len(class_distribution)), class_distribution)\n",
    "plt.xticks(range(len(class_distribution)), newsgroups_train.target_names, rotation=45)\n",
    "plt.title('Distribution of Classes in Training Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display distribution of classes in the test set\n",
    "y_test = newsgroups_test.target # Assign target labels from newsgroups_test to y_test\n",
    "class_distribution = np.bincount(y_test)\n",
    "plt.bar(range(len(class_distribution)), class_distribution)\n",
    "plt.xticks(range(len(class_distribution)), newsgroups_test.target_names, rotation=45)\n",
    "plt.title('Distribution of Classes in Test Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f9767",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Text data is converted into TF-IDF feature vectors. TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects how important a word is to a document in a collection. This step is crucial for converting text data into a format that can be used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d02ce",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "A Decision Tree classifier is initialized and trained on the processed training data. Decision Trees are a non-linear predictive modeling tool that can be used for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fd559",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "The trained model is used to make predictions on the test set, and the model's performance is evaluated using accuracy and a detailed classification report, which includes precision, recall, f1-score, and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups_test.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba1c80",
   "metadata": {},
   "source": [
    "The output demonstrates the performance of a Decision Tree classifier on a text classification task using the 20 Newsgroups dataset. An accuracy of approximately 63.25% indicates that the model correctly predicted the category of over half of the newsgroup posts in the test set. The precision, recall, and f1-score for each category show how well the model performs for individual classes. Precision indicates the model's accuracy in labeling a class correctly, recall reflects how well the model identifies all relevant instances of a class, and the f1-score provides a balance between precision and recall. The variation across different categories (alt.atheism, comp.graphics, sci.med, soc.religion.christian) suggests that the model's ability to correctly classify posts varies with the subject matter, performing best in 'soc.religion.christian' and worst in 'alt.atheism'.\n",
    "\n",
    "# Comparison with Other Text Classification Techniques\n",
    "\n",
    "We will compare decision trees with other popular text classification algorithms such as Random Forest and Support Vector Machines.\n",
    "\n",
    "# Text Classification using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups_test.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91f7cf",
   "metadata": {},
   "source": [
    "# Text Classification using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train an SVM classifier\n",
    "clf = SVC(kernel='linear', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups_test.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd78070",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "1.   SVM outperforms both Random Forest and Decision Tree classifiers in terms of accuracy and overall performance, as indicated by the higher F1-score.\n",
    "2.   Random Forest performs relatively well but slightly lags behind SVM.\n",
    "3. Decision Tree shows the lowest performance among the three classifiers, indicating the importance of choosing an appropriate algorithm for text classification tasks.\n",
    "\n",
    "\n",
    "Are you passionate about data and looking to make one giant leap into your career?\n",
    "\n",
    "Our Data Science Course will help you change your game and, most importantly, allow students, professionals, and working adults to tide over into the data science immersion.\n",
    "\n",
    "Master state-of-the-art methodologies, powerful tools, and industry best practices, hands-on projects, and real-world applications. Become the executive head of industries related to Data Analysis, Machine Learning, and Data Visualization with these growing skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data handling, model training, and evaluation\n",
    "# These libraries help load data, convert text to numbers, build models, and assess model accuracy.\n",
    "from sklearn.model_selection import train_test_split  # For dividing data into training and testing sets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For transforming text into numerical features\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree model for classification\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest model for classification\n",
    "from sklearn.svm import SVC  # Support Vector Machine model for classification\n",
    "from sklearn.metrics import classification_report, accuracy_score  # For measuring model performance\n",
    "from sklearn.datasets import fetch_20newsgroups  # Dataset containing news articles for text classification\n",
    "import numpy as np  # For numerical calculations\n",
    "import matplotlib.pyplot as plt  # For creating visual plots\n",
    "\n",
    "# Load the dataset\n",
    "# '20 Newsgroups' is a collection of news articles grouped by topic.\n",
    "# We select four categories (topics) to make this a simpler classification problem.\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "# Load training data (data that the model will learn from)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "# Load test data (data to test the model's accuracy after training)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Visualize the distribution of each category in the training set\n",
    "# We can see how many articles we have in each category (class), which helps us understand data balance.\n",
    "y_train = newsgroups_train.target  # Extract the category labels for the training data\n",
    "class_distribution_train = np.bincount(y_train)  # Count the number of articles per category\n",
    "plt.bar(range(len(class_distribution_train)), class_distribution_train)  # Create a bar plot for visual representation\n",
    "plt.xticks(range(len(class_distribution_train)), newsgroups_train.target_names, rotation=45)  # Label each category\n",
    "plt.title('Training Set Class Distribution')  # Title of the plot\n",
    "plt.xlabel('Class')  # Label for x-axis (category)\n",
    "plt.ylabel('Number of Documents')  # Label for y-axis (count of documents)\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# Visualize the distribution of each category in the test set\n",
    "# This is done to ensure that test data is also balanced like the training data.\n",
    "y_test = newsgroups_test.target  # Extract category labels for the test data\n",
    "class_distribution_test = np.bincount(y_test)  # Count articles per category in test data\n",
    "plt.bar(range(len(class_distribution_test)), class_distribution_test)  # Bar plot for visual representation\n",
    "plt.xticks(range(len(class_distribution_test)), newsgroups_test.target_names, rotation=45)  # Label each category\n",
    "plt.title('Test Set Class Distribution')  # Title of the plot\n",
    "plt.xlabel('Class')  # Label for x-axis (category)\n",
    "plt.ylabel('Number of Documents')  # Label for y-axis (count of documents)\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# Preprocess text data: Convert text into numerical data using TF-IDF\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) measures how important words are to a document\n",
    "# by assigning weights to each word, based on their frequency in a document and in the entire dataset.\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Initialize vectorizer, removing common English stop words\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)  # Learn vocabulary from training data and transform it\n",
    "X_test = vectorizer.transform(newsgroups_test.data)  # Transform test data using the learned vocabulary\n",
    "# Here, X_train and X_test are now matrices with numerical values representing the text data.\n",
    "\n",
    "# Model 1: Decision Tree Classifier\n",
    "# Decision Tree builds a flowchart-like model of decisions to classify data into categories.\n",
    "clf_tree = DecisionTreeClassifier(random_state=42)  # Initialize Decision Tree with a random seed for consistency\n",
    "clf_tree.fit(X_train, y_train)  # Train the model using the training data\n",
    "\n",
    "# Predict and evaluate the Decision Tree model\n",
    "y_pred_tree = clf_tree.predict(X_test)  # Use the trained model to predict labels for the test data\n",
    "print(\"Decision Tree Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))  # Show the accuracy (percentage of correct predictions)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tree, target_names=newsgroups_test.target_names))\n",
    "# The classification report shows performance for each category, including precision and recall.\n",
    "\n",
    "# Model 2: Random Forest Classifier\n",
    "# A Random Forest builds multiple Decision Trees and averages their predictions for better accuracy.\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)  # Initialize Random Forest with 100 trees\n",
    "clf_rf.fit(X_train, y_train)  # Train the model using the training data\n",
    "\n",
    "# Predict and evaluate the Random Forest model\n",
    "y_pred_rf = clf_rf.predict(X_test)  # Predict labels for the test data\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf, target_names=newsgroups_test.target_names))\n",
    "\n",
    "# Model 3: Support Vector Machine (SVM) Classifier\n",
    "# SVM finds the best boundary that separates categories, maximizing the distance between different categories.\n",
    "clf_svc = SVC(kernel='linear', random_state=42)  # Initialize SVM with a linear kernel (suitable for text data)\n",
    "clf_svc.fit(X_train, y_train)  # Train the model using the training data\n",
    "\n",
    "# Predict and evaluate the SVM model\n",
    "y_pred_svc = clf_svc.predict(X_test)  # Predict labels for the test data\n",
    "print(\"SVM Model Performance:\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svc, target_names=newsgroups_test.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d43c73",
   "metadata": {},
   "source": [
    "### Step-by-Step Explanation with Comments\n",
    "\n",
    "1. **Library Imports**: Detailed comments explain the purpose of each library, especially for someone new to Python and machine learning.\n",
    "\n",
    "2. **Loading and Understanding the Dataset**:\n",
    "   - Describes the `20 Newsgroups` dataset and why specific categories were chosen.\n",
    "   - Explains the difference between training and test data and why both are needed.\n",
    "\n",
    "3. **Class Distribution Visualization**:\n",
    "   - Step-by-step comments guide the reader through checking data balance, which is essential for fair model training.\n",
    "\n",
    "4. **TF-IDF Vectorization**:\n",
    "   - Simplified explanation of how TF-IDF works to convert text data into numbers.\n",
    "   - Notes why removing stop words (common words) helps improve model focus on significant terms.\n",
    "\n",
    "5. **Models and Their Descriptions**:\n",
    "   - **Decision Tree**: Describes the basics of how a Decision Tree makes predictions.\n",
    "   - **Random Forest**: Explains why combining multiple Decision Trees improves performance.\n",
    "   - **SVM**: Highlights the core idea of finding a boundary to separate categories.\n",
    "\n",
    "6. **Evaluation and Metrics**:\n",
    "   - Explains `accuracy_score` for measuring overall accuracy.\n",
    "   - Introduces the `classification_report`, which gives details on precision and recall for each category.\n",
    "\n",
    "This should make the code easy to understand, with each concept introduced gradually to guide a beginner through the process of building and evaluating classification models for text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e93c1",
   "metadata": {},
   "source": [
    "# Using Decision Trees for spam detection is a straightforward and effective approach.\n",
    "\n",
    "# Here's a step-by-step guide on how you can implement this in a simple way, using a basic understanding of the process involved in text classification.\n",
    "\n",
    "### Steps to Implement Spam Detection Using Decision Trees\n",
    "\n",
    "1. **Gather Your Data**:\n",
    "   - You need a dataset of emails (or text messages) labeled as either \"spam\" or \"not spam.\" You can use existing datasets like the [SMS Spam Collection](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) or the [Enron Email Dataset](https://www.cs.cmu.edu/~enron/).\n",
    "   - Ensure your dataset contains two main columns: the text of the message and the label (spam or not spam).\n",
    "\n",
    "2. **Prepare Your Environment**:\n",
    "   - Make sure you have Python installed along with libraries like `scikit-learn`, `pandas`, and `matplotlib`. You can install these using pip:\n",
    "     ```bash\n",
    "     pip install scikit-learn pandas matplotlib\n",
    "     ```\n",
    "\n",
    "3. **Load and Preprocess Your Data**:\n",
    "   - Use pandas to load your data and preprocess it (cleaning and formatting).\n",
    "   - Split your data into training and testing sets to evaluate your model later.\n",
    "\n",
    "4. **Convert Text to Numerical Format**:\n",
    "   - Use `TfidfVectorizer` to convert the text messages into numerical format, which is required for the Decision Tree model.\n",
    "\n",
    "5. **Train the Decision Tree Model**:\n",
    "   - Create and train your Decision Tree classifier using the training data.\n",
    "\n",
    "6. **Make Predictions and Evaluate Your Model**:\n",
    "   - Use the trained model to predict whether the messages in the test set are spam or not.\n",
    "   - Evaluate the model's performance using metrics like accuracy, precision, and recall.\n",
    "\n",
    "### Example Code for Spam Detection Using Decision Trees\n",
    "\n",
    "Here’s how you can implement the above steps in Python:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # To convert text to numerical features\n",
    "from sklearn.tree import DecisionTreeClassifier  # For the Decision Tree model\n",
    "from sklearn.metrics import classification_report, accuracy_score  # To evaluate model performance\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "# Replace 'spam_data.csv' with your actual file path\n",
    "data = pd.read_csv('spam_data.csv')  # Load the dataset\n",
    "print(data.head())  # Display the first few rows to understand the data structure\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Assuming the dataset has 'text' and 'label' columns\n",
    "X = data['text']  # Features (text messages)\n",
    "y = data['label']  # Labels (spam or not spam)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Convert text data to numerical format\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Initialize the vectorizer\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_vectorized = vectorizer.transform(X_test)  # Transform the test data\n",
    "\n",
    "# Step 4: Train the Decision Tree model\n",
    "clf = DecisionTreeClassifier(random_state=42)  # Initialize the model\n",
    "clf.fit(X_train_vectorized, y_train)  # Train the model with the training data\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_vectorized)  # Predict labels for the test data\n",
    "\n",
    "# Step 6: Evaluate the model's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Print the accuracy\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))  # Detailed performance metrics\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "1. **Loading Data**: The code starts by loading your dataset from a CSV file into a pandas DataFrame. You can check the structure of your data with `print(data.head())`.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - The features (`X`) are the text messages, and the labels (`y`) indicate whether they are spam or not.\n",
    "   - The data is split into training and testing sets to help evaluate the model later.\n",
    "\n",
    "3. **Text Vectorization**: `TfidfVectorizer` converts the text data into a numerical format that the Decision Tree can understand. It removes common words (stop words) that don't contribute much to the classification.\n",
    "\n",
    "4. **Model Training**: A Decision Tree classifier is created and trained using the training data.\n",
    "\n",
    "5. **Prediction**: The trained model predicts whether messages in the test set are spam or not.\n",
    "\n",
    "6. **Evaluation**: Finally, the model’s accuracy and detailed classification metrics are printed to evaluate its performance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "By following these steps, you can effectively use the Decision Trees method for spam detection. This approach helps automate the identification of spam messages based on the text content, which can be very useful in real-world applications like email filtering and messaging apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c745c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # To convert text to numerical features\n",
    "from sklearn.tree import DecisionTreeClassifier  # For the Decision Tree model\n",
    "from sklearn.metrics import classification_report, accuracy_score  # To evaluate model performance\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "# Replace 'spam_data.csv' with your actual file path\n",
    "data = pd.read_csv('spam_data.csv')  # Load the dataset\n",
    "print(data.head())  # Display the first few rows to understand the data structure\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Assuming the dataset has 'text' and 'label' columns\n",
    "X = data['text']  # Features (text messages)\n",
    "y = data['label']  # Labels (spam or not spam)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Convert text data to numerical format\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Initialize the vectorizer\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_vectorized = vectorizer.transform(X_test)  # Transform the test data\n",
    "\n",
    "# Step 4: Train the Decision Tree model\n",
    "clf = DecisionTreeClassifier(random_state=42)  # Initialize the model\n",
    "clf.fit(X_train_vectorized, y_train)  # Train the model with the training data\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_vectorized)  # Predict labels for the test data\n",
    "\n",
    "# Step 6: Evaluate the model's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Print the accuracy\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))  # Detailed performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d99a73",
   "metadata": {},
   "source": [
    "Running this code on Google Colab with the dataset stored in Google Drive requires mounting Google Drive and loading the dataset directly from there. I'll guide you through these steps and also make improvements to the code for better readability, modularity, and efficiency.\n",
    "\n",
    "Here’s the optimized code for the SMS Spam Collection dataset, which you can run on Google Colab. I’ll include detailed comments to guide you.\n",
    "\n",
    "### Step-by-Step Code for Google Colab\n",
    "\n",
    "```python\n",
    "# Step 1: Mount Google Drive to Access Dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Mounts Google Drive for file access\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "import pandas as pd  # Data manipulation\n",
    "from sklearn.model_selection import train_test_split  # Train/test split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Text to numerical features\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree model\n",
    "from sklearn.metrics import classification_report, accuracy_score  # Model evaluation\n",
    "\n",
    "# Step 3: Load the SMS Spam Collection dataset\n",
    "# Update the path below with the actual path to the dataset in your Google Drive\n",
    "file_path = '/content/drive/My Drive/path_to_your_file/SMSSpamCollection'  # Example path\n",
    "data = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'text'])  # Load dataset with tab separator\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())  # Show first few rows for verification\n",
    "\n",
    "# Step 4: Encode the labels\n",
    "# Convert 'spam' to 1 and 'ham' to 0 to make labels numeric for the model\n",
    "data['label'] = data['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "# Step 5: Split data into features and labels\n",
    "X = data['text']  # Text messages\n",
    "y = data['label']  # Labels (1 for spam, 0 for ham)\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(X_train)} messages\")\n",
    "print(f\"Testing set size: {len(X_test)} messages\")\n",
    "\n",
    "# Step 6: Convert text data to numerical format using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)  # Limit features to 3000 for efficiency\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)  # Fit & transform training data\n",
    "X_test_vectorized = vectorizer.transform(X_test)  # Transform test data\n",
    "\n",
    "# Step 7: Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=20)  # Adding max_depth to prevent overfitting\n",
    "clf.fit(X_train_vectorized, y_train)  # Train the model\n",
    "\n",
    "# Step 8: Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_vectorized)  # Predict spam or ham for the test set\n",
    "\n",
    "# Step 9: Evaluate the model's performance\n",
    "print(\"\\nDecision Tree Model Performance on SMS Spam Collection Dataset:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Show accuracy\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
    "```\n",
    "\n",
    "### Code Enhancements and Explanations:\n",
    "\n",
    "1. **Mount Google Drive**:\n",
    "   - Using `drive.mount()` allows access to files stored in Google Drive directly from Colab.\n",
    "\n",
    "2. **TF-IDF Vectorizer**:\n",
    "   - Specified `max_features=3000` in `TfidfVectorizer` to limit the number of features and make the model more efficient. This can help reduce memory usage and prevent overfitting.\n",
    "\n",
    "3. **Decision Tree Model Parameters**:\n",
    "   - Added `max_depth=20` in `DecisionTreeClassifier` to limit the depth of the tree. This helps prevent overfitting by avoiding a model that is too complex for the dataset.\n",
    "\n",
    "4. **Detailed Output**:\n",
    "   - The code prints the model’s accuracy and provides a classification report, which includes precision, recall, and F1-score for both spam and ham classes.\n",
    "\n",
    "### Additional Tips for Running in Google Colab:\n",
    "\n",
    "- **Change File Path**: Ensure `file_path` points correctly to the SMS Spam Collection dataset within your Google Drive.\n",
    "- **Install Missing Packages**: If needed, you can install any missing packages in Colab using `!pip install package_name`.\n",
    "  \n",
    "This setup should make it straightforward to load your dataset from Google Drive, preprocess the text data, train a Decision Tree classifier, and evaluate its performance on spam detection in a Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee951a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive to Access Dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Mounts Google Drive for file access\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "import pandas as pd  # Data manipulation\n",
    "from sklearn.model_selection import train_test_split  # Train/test split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Text to numerical features\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree model\n",
    "from sklearn.metrics import classification_report, accuracy_score  # Model evaluation\n",
    "\n",
    "# Step 3: Load the SMS Spam Collection dataset\n",
    "# Replace with the actual path to your dataset in Google Drive\n",
    "file_path = '/content/drive/MyDrive/dataset/SMSSpamCollection'  # Example: if your file is in 'MyDrive/spam_data.csv'\n",
    "data = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'text'])  # Load dataset with tab separator\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())  # Show first few rows for verification\n",
    "\n",
    "# Step 4: Encode the labels\n",
    "# Convert 'spam' to 1 and 'ham' to 0 to make labels numeric for the model\n",
    "data['label'] = data['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "# Step 5: Split data into features and labels\n",
    "X = data['text']  # Text messages\n",
    "y = data['label']  # Labels (1 for spam, 0 for ham)\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(X_train)} messages\")\n",
    "print(f\"Testing set size: {len(X_test)} messages\")\n",
    "\n",
    "# Step 6: Convert text data to numerical format using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)  # Limit features to 3000 for efficiency\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)  # Fit & transform training data\n",
    "X_test_vectorized = vectorizer.transform(X_test)  # Transform test data\n",
    "\n",
    "# Step 7: Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=20)  # Adding max_depth to prevent overfitting\n",
    "clf.fit(X_train_vectorized, y_train)  # Train the model\n",
    "\n",
    "# Step 8: Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_vectorized)  # Predict spam or ham for the test set\n",
    "\n",
    "# Step 9: Evaluate the model's performance\n",
    "print(\"\\nDecision Tree Model Performance on SMS Spam Collection Dataset:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Show accuracy\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive\n",
    "# Google Colab requires mounting Google Drive if we want to use files from it.\n",
    "# This code connects to Google Drive to access the dataset stored there.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # This command mounts Google Drive so we can access files within it.\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "# Libraries are reusable pieces of code that help with specific tasks.\n",
    "import pandas as pd  # 'pandas' helps to manage and analyze data in table format.\n",
    "from sklearn.model_selection import train_test_split  # Splits data into training and testing sets.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Converts text into numbers for analysis.\n",
    "from sklearn.tree import DecisionTreeClassifier  # The Decision Tree model that will predict spam or ham.\n",
    "from sklearn.metrics import classification_report, accuracy_score  # Tools to measure model performance.\n",
    "\n",
    "# Step 3: Load the SMS Spam Collection dataset\n",
    "# Here, we load the dataset containing SMS messages and their labels (spam or ham).\n",
    "file_path = '/content/drive/MyDrive/dataset/SMSSpamCollection'  # This path should be updated to your dataset location on Google Drive.\n",
    "\n",
    "# Read the dataset using 'pandas' with tab ('\\t') as the separator.\n",
    "# Since this dataset doesn’t have column names, we name them 'label' and 'text' for clarity.\n",
    "data = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "# Display the first few rows to confirm the data loaded correctly\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())  # Show the first few rows of data to get an idea of its structure.\n",
    "\n",
    "# Step 4: Encode the labels as numbers\n",
    "# Machine learning models work with numbers, so we need to convert text labels (spam or ham) to numbers.\n",
    "# Here, we map 'spam' to 1 and 'ham' to 0.\n",
    "data['label'] = data['label'].map({'spam': 1, 'ham': 0})  # 'spam' becomes 1, and 'ham' becomes 0.\n",
    "\n",
    "# Step 5: Define features (X) and labels (y)\n",
    "# - X will contain the text messages (SMS content).\n",
    "# - y will contain the corresponding labels (1 for spam, 0 for ham).\n",
    "X = data['text']  # Features (the actual text of the SMS messages)\n",
    "y = data['label']  # Labels (1 for spam, 0 for ham)\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "# We need to divide the data so we can train the model on one part and test it on another.\n",
    "# - The model will learn patterns in the training set (80% of data).\n",
    "# - The testing set (20% of data) will help us see how well the model performs on unseen messages.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the training and testing sets\n",
    "print(f\"Training set size: {len(X_train)} messages\")  # Number of messages in training set\n",
    "print(f\"Testing set size: {len(X_test)} messages\")  # Number of messages in testing set\n",
    "\n",
    "# Step 7: Convert text data into a numerical format using TF-IDF Vectorizer\n",
    "# The model can't understand text directly, so we need to turn it into numbers.\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) assigns a numerical value to words.\n",
    "# This helps the model focus on words that are important in distinguishing spam from ham.\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)  # Ignores common words and limits to 3000 words for efficiency.\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)  # 'fit' learns vocabulary from training data, 'transform' converts it to numbers.\n",
    "print(X_train_vectorized)\n",
    "# Transform the testing data\n",
    "X_test_vectorized = vectorizer.transform(X_test)  # Convert test messages to numbers using the same vocabulary.\n",
    "\n",
    "# Step 8: Train the Decision Tree Classifier\n",
    "# Now we create and train our Decision Tree model.\n",
    "# The Decision Tree works by splitting data based on certain conditions to classify messages as spam or ham.\n",
    "# We set 'max_depth' to limit the depth of the tree, helping it generalize better and avoid overfitting.\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=20)  # Setting 'max_depth' keeps the model simpler and faster.\n",
    "\n",
    "# Train (fit) the model on the training data\n",
    "clf.fit(X_train_vectorized, y_train)  # The model learns from training data to recognize patterns in spam and ham.\n",
    "\n",
    "# Step 9: Make predictions on the test set\n",
    "# With our trained model, we now predict whether each message in the test set is spam or ham.\n",
    "y_pred = clf.predict(X_test_vectorized)  # Predict labels (spam/ham) for test messages\n",
    "\n",
    "# Step 10: Evaluate the model's performance\n",
    "# To see how well the model performs, we check its accuracy and other metrics.\n",
    "# - Accuracy: Percentage of correct predictions (out of all predictions).\n",
    "# - Classification report: Detailed performance metrics for spam and ham (precision, recall, F1-score).\n",
    "print(\"\\nDecision Tree Model Performance on SMS Spam Collection Dataset:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Shows overall accuracy of the model\n",
    "\n",
    "# Print a classification report for further insights into model performance on each class (ham/spam)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb70a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize a dictionary to store the results for each depth\n",
    "depth_results = {}\n",
    "\n",
    "# Test decision tree classifiers with varying depths\n",
    "for depth in range(1, 21):  # Test depths from 1 to 20\n",
    "    clf = DecisionTreeClassifier(random_state=42, max_depth=depth)\n",
    "    clf.fit(X_train_vectorized, y_train)  # Train the classifier\n",
    "    y_pred = clf.predict(X_test_vectorized)  # Predict on the test set\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    depth_results[depth] = accuracy  # Store accuracy for the current depth\n",
    "\n",
    "    print(f\"Depth: {depth}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(depth_results.keys()), list(depth_results.values()), marker='o', color='blue')\n",
    "plt.title('Effect of Tree Depth on Accuracy', fontsize=14)\n",
    "plt.xlabel('Tree Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 21))\n",
    "plt.show()\n",
    "\n",
    "# Identify the best depth\n",
    "best_depth = max(depth_results, key=depth_results.get)\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Accuracy: {depth_results[best_depth]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize a dictionary to store the results for each depth\n",
    "depth_results = {}\n",
    "\n",
    "# Test decision tree classifiers with varying depths\n",
    "for depth in range(1, 101):  # Test depths from 1 to 100\n",
    "    clf = DecisionTreeClassifier(random_state=42, max_depth=depth)\n",
    "    clf.fit(X_train_vectorized, y_train)  # Train the classifier\n",
    "    y_pred = clf.predict(X_test_vectorized)  # Predict on the test set\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    depth_results[depth] = accuracy  # Store accuracy for the current depth\n",
    "\n",
    "    # Print accuracy for every 10th depth to reduce verbosity\n",
    "    if depth % 10 == 0 or depth == 1 or depth == 100:\n",
    "        print(f\"Depth: {depth}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(depth_results.keys()), list(depth_results.values()), marker='o', color='blue')\n",
    "plt.title('Effect of Tree Depth on Accuracy (1-100)', fontsize=14)\n",
    "plt.xlabel('Tree Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.xticks(range(0, 101, 10))  # Show x-axis ticks every 10 depths\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(f\"Depth 1-10: Likely to show rapid improvement in accuracy as the tree learns basic patterns.\")\n",
    "print(f\"Depth 10-100: Accuracy seems to not have a significant change due to overfitting on the training data.\")\n",
    "print(f\"Optimal Depth: The depth with the highest accuracy.\")\n",
    "\n",
    "# Identify the best depth\n",
    "best_depth = max(depth_results, key=depth_results.get)\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Accuracy: {depth_results[best_depth]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b17c31",
   "metadata": {},
   "source": [
    "NEXT WEEK TASKS:\n",
    "\n",
    "1.   In one figure we should have accuracy of training and test dataset together.\n",
    "\n",
    "2.   Check actual depth for decision tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb81c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize dictionaries to store training and testing accuracies\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "# Test decision tree classifiers with varying depths\n",
    "for depth in range(1, 21):  # Test depths from 1 to 20\n",
    "    clf = DecisionTreeClassifier(random_state=42, max_depth=depth)\n",
    "    clf.fit(X_train_vectorized, y_train)  # Train the classifier\n",
    "\n",
    "    # Predict on the training set\n",
    "    y_train_pred = clf.predict(X_train_vectorized)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)  # Training accuracy\n",
    "    train_accuracies[depth] = train_accuracy\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_test_pred = clf.predict(X_test_vectorized)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)  # Testing accuracy\n",
    "    test_accuracies[depth] = test_accuracy\n",
    "\n",
    "    print(f\"Depth: {depth}, Training Accuracy: {train_accuracy:.4f}, Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(train_accuracies.keys()), list(train_accuracies.values()), marker='o', label='Training Accuracy', color='blue')\n",
    "plt.plot(list(test_accuracies.keys()), list(test_accuracies.values()), marker='s', label='Testing Accuracy', color='orange')\n",
    "plt.title('Effect of Tree Depth on Accuracy', fontsize=14)\n",
    "plt.xlabel('Tree Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 21))\n",
    "plt.show()\n",
    "\n",
    "# Identify the best depth based on test accuracy\n",
    "best_depth = max(test_accuracies, key=test_accuracies.get)\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Testing Accuracy: {test_accuracies[best_depth]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f092e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 3: Load the SMS Spam Collection dataset\n",
    "file_path = '/content/drive/MyDrive/dataset/SMSSpamCollection'  # Update the path to your dataset\n",
    "data = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'text'])\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 4: Encode the labels as numbers\n",
    "data['label'] = data['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "# Step 5: Define features (X) and labels (y)\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(X_train)} messages\")\n",
    "print(f\"Testing set size: {len(X_test)} messages\")\n",
    "\n",
    "# Step 7: Convert text data into a numerical format using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 8: Train and evaluate the Decision Tree Classifier for varying depths\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "for depth in range(1, 21):  # Test depths from 1 to 20\n",
    "    clf = DecisionTreeClassifier(random_state=42, max_depth=depth)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Predict on the training set\n",
    "    y_train_pred = clf.predict(X_train_vectorized)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_accuracies[depth] = train_accuracy\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_test_pred = clf.predict(X_test_vectorized)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_accuracies[depth] = test_accuracy\n",
    "\n",
    "    print(f\"Depth: {depth}, Training Accuracy: {train_accuracy:.4f}, Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Step 9: Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(train_accuracies.keys()), list(train_accuracies.values()), marker='o', label='Training Accuracy', color='blue')\n",
    "plt.plot(list(test_accuracies.keys()), list(test_accuracies.values()), marker='s', label='Testing Accuracy', color='orange')\n",
    "plt.title('Effect of Tree Depth on Accuracy', fontsize=14)\n",
    "plt.xlabel('Tree Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 21))\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Identify the best depth based on test accuracy\n",
    "best_depth = max(test_accuracies, key=test_accuracies.get)\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Testing Accuracy: {test_accuracies[best_depth]:.4f}\")\n",
    "\n",
    "# Step 11: Evaluate the final model at the best depth\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=best_depth)\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "y_test_pred = clf.predict(X_test_vectorized)\n",
    "\n",
    "print(\"\\nFinal Model Performance at Best Depth:\")\n",
    "print(\"Accuracy on Test Set:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred, target_names=['Ham', 'Spam']))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
